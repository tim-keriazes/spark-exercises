{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "In this lesson, we will acquire and prepare the data we will use in the rest of this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "from env import get_db_url\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "Spark lets us read data in from a variety of data sources using what it calls a `DataFrameReader`. We can access the `read` property of our `spark` object and then set various options and read from a data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/source.csv\", sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------------+\n",
      "|index|source_id| source_username|\n",
      "+-----+---------+----------------+\n",
      "|    0|   100137|Merlene Blodgett|\n",
      "|    1|   103582|     Carmen Cura|\n",
      "|    2|   106463| Richard Sanchez|\n",
      "+-----+---------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = get_db_url(\"311_data\")\n",
    "query = \"SELECT * FROM source;\"\n",
    "df = pd.read_sql(query, url)\n",
    "df = spark.createDataFrame(df)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code could also be written like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, source_username: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"data/source.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Schemas\n",
    "\n",
    "Spark includes a concept of a *data schema*, which is a way to specify the types of our data ahead of time. Doing so lets us be sure about the structure of our data, and can significantly increase the speed of loading data (inferring the schema can be a costly operation for large datasets).\n",
    "\n",
    "We'll import several things from the `pyspark.sql.types` module:\n",
    "\n",
    "- `StringType`\n",
    "- `DoubleType`\n",
    "- `IntegerType`\n",
    "- `LongType`\n",
    "- `ShortType`\n",
    "- `TimestampType`\n",
    "- `FloatType`\n",
    "- `DateType`\n",
    "\n",
    "All of the above types will go inside of a `StructField`, which will be encapsulated in a `StructType`, and the resulting object will represent our data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, source_username: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"source_id\", StringType()),\n",
    "        StructField(\"source_username\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "spark.read.csv(\"data/source.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that instead of `inferSchema=True`, we pass the `schema` object in to the `read.csv` call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data\n",
    "\n",
    "A spark dataframe can be written to a local destination using the `.write` property. Several common output formats are:\n",
    "\n",
    "- `csv`: for writing to a local csv file(s)\n",
    "- `parquet`: [Parquet](https://parquet.apache.org/) is a very popular columnar storage format for Hadoop.\n",
    "- `json`: for writing to a local json file(s)\n",
    "- `jdbc`: for writing to a SQL database table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for demo purposes\n",
    "from pydataset import data\n",
    "\n",
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "\n",
    "mpg.write.json(\"data/mpg_json\", mode=\"overwrite\")\n",
    "\n",
    "# like much else in spark, there's multiple ways we could do this:\n",
    "(\n",
    "    mpg.write.format(\"csv\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(\"data/mpg_csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For the rest of this lesson, we'll take a look at the `case` data from the San Antonio 311 calls dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 1/1/18 0:42          \n",
      " case_closed_date     | 1/1/18 12:29         \n",
      " SLA_due_date         | 9/26/20 0:42         \n",
      " case_late            | NO                   \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | YES                  \n",
      " dept_division        | Field Operations     \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  EL PASO ST,... \n",
      " council_district     | 5                    \n",
      "-RECORD 1------------------------------------\n",
      " case_id              | 1014127333           \n",
      " case_opened_date     | 1/1/18 0:46          \n",
      " case_closed_date     | 1/3/18 8:11          \n",
      " SLA_due_date         | 1/5/18 8:30          \n",
      " case_late            | NO                   \n",
      " num_days_late        | -2.0126041669999997  \n",
      " case_closed          | YES                  \n",
      " dept_division        | Storm Water          \n",
      " service_request_type | Removal Of Obstru... \n",
      " SLA_days             | 4.322222222          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 2215  GOLIAD RD, ... \n",
      " council_district     | 3                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/case.csv\", header=True, inferSchema=True)\n",
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now cover various pieces of data preparation we wish to do and how to do those with spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns\n",
    "\n",
    "We'll rename this column to match with the other date-type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"SLA_due_date\", \"case_due_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two columns, `case_closed` and `case_late` store yes/no values. Currently spark thinks they are strings; let's turn them into booleans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------+\n",
      "|case_closed|case_late| count|\n",
      "+-----------+---------+------+\n",
      "|         NO|      YES|  6525|\n",
      "|        YES|      YES| 87978|\n",
      "|         NO|       NO| 11585|\n",
      "|        YES|       NO|735616|\n",
      "+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# demonstrating we only have yes/no in each field\n",
    "df.groupBy(\"case_closed\", \"case_late\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|case_closed|case_late|\n",
      "+-----------+---------+\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|     true|\n",
      "+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"case_closed\", expr('case_closed == \"YES\"')).withColumn(\n",
    "    \"case_late\", expr('case_late == \"YES\"')\n",
    ")\n",
    "\n",
    "df.select(\"case_closed\", \"case_late\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `council_district` column appears as though it is an integer, but this is just a unique identifier for each district, that is, we aren't going to be performing arithmetic with this number, so we will turn it into a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|council_district| count|\n",
      "+----------------+------+\n",
      "|               1|119309|\n",
      "|               6| 74095|\n",
      "|               3|102706|\n",
      "|               5|114609|\n",
      "|               9| 40916|\n",
      "|               4| 93778|\n",
      "|               8| 42345|\n",
      "|               7| 72445|\n",
      "|              10| 62926|\n",
      "|               2|114745|\n",
      "|               0|  3830|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"council_district\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"council_district\", col(\"council_district\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will handle the 3 columns that have dates in them. We'll use spark's  `to_timestamp` function for this.\n",
    "\n",
    "In order to work properly, we'll need to provide the date format when using `to_timestamp`. The date format is a little different than the date functionality we've worked with in pandas, this is becuase it is using [Java's `SimpleDateFormat`][1].\n",
    "\n",
    "[1]: https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before handling dates\n",
      "+----------------+----------------+-------------+\n",
      "|case_opened_date|case_closed_date|case_due_date|\n",
      "+----------------+----------------+-------------+\n",
      "|     1/1/18 0:42|    1/1/18 12:29| 9/26/20 0:42|\n",
      "|     1/1/18 0:46|     1/3/18 8:11|  1/5/18 8:30|\n",
      "|     1/1/18 0:48|     1/2/18 7:57|  1/5/18 8:30|\n",
      "|     1/1/18 1:29|     1/2/18 8:13| 1/17/18 8:30|\n",
      "|     1/1/18 1:34|    1/1/18 13:29|  1/1/18 4:34|\n",
      "+----------------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--- After\n",
      "+-------------------+-------------------+-------------------+\n",
      "|   case_opened_date|   case_closed_date|      case_due_date|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2018-01-01 00:42:00|2018-01-01 00:42:00|2018-01-01 00:42:00|\n",
      "|2018-01-01 00:46:00|2018-01-01 00:46:00|2018-01-01 00:46:00|\n",
      "|2018-01-01 00:48:00|2018-01-01 00:48:00|2018-01-01 00:48:00|\n",
      "|2018-01-01 01:29:00|2018-01-01 01:29:00|2018-01-01 01:29:00|\n",
      "|2018-01-01 01:34:00|2018-01-01 01:34:00|2018-01-01 01:34:00|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Before handling dates\")\n",
    "df.select(\"case_opened_date\", \"case_closed_date\", \"case_due_date\").show(5)\n",
    "\n",
    "fmt = \"M/d/yy H:mm\"\n",
    "df = (\n",
    "    df.withColumn(\"case_opened_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    "    .withColumn(\"case_closed_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    "    .withColumn(\"case_due_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    ")\n",
    "\n",
    "print(\"--- After\")\n",
    "df.select(\"case_opened_date\", \"case_closed_date\", \"case_due_date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations\n",
    "\n",
    "Now that we have everything stored as the correct data type, we will make a few transformations to the data.\n",
    "\n",
    "We'll begin by normalizing the request address field. Using the `trim` and `lower` functions lets us strip any leading or trailing whitespace and convert everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before\n",
      "+--------------------+\n",
      "|     request_address|\n",
      "+--------------------+\n",
      "|2315  EL PASO ST,...|\n",
      "|2215  GOLIAD RD, ...|\n",
      "|102  PALFREY ST W...|\n",
      "|114  LA GARDE ST,...|\n",
      "|734  CLEARVIEW DR...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--- After\n",
      "+--------------------+\n",
      "|     request_address|\n",
      "+--------------------+\n",
      "|2315  el paso st,...|\n",
      "|2215  goliad rd, ...|\n",
      "|102  palfrey st w...|\n",
      "|114  la garde st,...|\n",
      "|734  clearview dr...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Before\")\n",
    "df.select(\"request_address\").show(5)\n",
    "\n",
    "df = df.withColumn(\"request_address\", trim(lower(df.request_address)))\n",
    "\n",
    "print(\"--- After\")\n",
    "df.select(\"request_address\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will convert the number of days a case is late to a number of weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|      num_days_late|      num_weeks_late|\n",
      "+-------------------+--------------------+\n",
      "| -998.5087616000001|        -142.6441088|\n",
      "|-2.0126041669999997|-0.28751488099999994|\n",
      "|       -3.022337963|-0.43176256614285713|\n",
      "|       -15.01148148| -2.1444973542857144|\n",
      "|0.37216435200000003|         0.053166336|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\n",
    "    \"num_weeks_late\", expr(\"num_days_late / 7 AS num_weeks_late\")\n",
    ")\n",
    "\n",
    "df.select(\"num_days_late\", \"num_weeks_late\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can format the council district column a little differently. We'll add leading 0s to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|council_district|\n",
      "+----------------+\n",
      "|             005|\n",
      "|             003|\n",
      "|             003|\n",
      "|             003|\n",
      "|             007|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"council_district\", col(\"council_district\").cast(\"int\"))\n",
    "\n",
    "# '%03d' means at least 3 digits, pad with 0s\n",
    "#\n",
    "# In order to use the format_string function the way we are, we'll need to\n",
    "# convert council_district back to an integer temporarily, but the final output\n",
    "# will be a string.\n",
    "df = df.withColumn(\n",
    "    \"council_district\",\n",
    "    format_string(\"%03d\", col(\"council_district\").cast(\"int\")),\n",
    ")\n",
    "\n",
    "df.select(\"council_district\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features\n",
    "\n",
    "Let's now create some new features based on our existing data.\n",
    "\n",
    "We will first extract the zipcode from the address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|zipcode|\n",
      "+-------+\n",
      "|  78207|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78228|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"zipcode\", regexp_extract(\"request_address\", r\"\\d+$\", 0))\n",
    "\n",
    "df.select(\"zipcode\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined the zipcode as the last sequence of digits at the end of the string.\n",
    "\n",
    "Next we will create several new, related columns:\n",
    "\n",
    "- `case_age`: How old the case is; the difference in days between when the case was opened and the current day\n",
    "- `days_to_closed`: The number of days between when the case was opened and when it was closed\n",
    "- `case_lifetime`: Number of days between when the case was opened and when it was closed, if the case is still open, the number of days since the case was opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|case_closed|   case_opened_date|   case_closed_date|case_age|days_to_closed|case_lifetime|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|       true|2018-01-01 00:42:00|2018-01-01 00:42:00|     707|             0|            0|\n",
      "|       true|2018-01-01 00:46:00|2018-01-01 00:46:00|     707|             0|            0|\n",
      "|       true|2018-01-01 00:48:00|2018-01-01 00:48:00|     707|             0|            0|\n",
      "|       true|2018-01-01 01:29:00|2018-01-01 01:29:00|     707|             0|            0|\n",
      "|       true|2018-01-01 01:34:00|2018-01-01 01:34:00|     707|             0|            0|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|case_closed|   case_opened_date|   case_closed_date|case_age|days_to_closed|case_lifetime|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|      false|2018-01-02 09:39:00|2018-01-02 09:39:00|     706|             0|          706|\n",
      "|      false|2018-01-02 10:49:00|2018-01-02 10:49:00|     706|             0|          706|\n",
      "|      false|2018-01-02 13:45:00|2018-01-02 13:45:00|     706|             0|          706|\n",
      "|      false|2018-01-02 14:09:00|2018-01-02 14:09:00|     706|             0|          706|\n",
      "|      false|2018-01-02 14:34:00|2018-01-02 14:34:00|     706|             0|          706|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    df.withColumn(\n",
    "        \"case_age\", datediff(current_timestamp(), \"case_opened_date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"days_to_closed\", datediff(\"case_closed_date\", \"case_opened_date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"case_lifetime\",\n",
    "        when(expr(\"! case_closed\"), col(\"case_age\")).otherwise(\n",
    "            col(\"days_to_closed\")\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "df.select(\n",
    "    \"case_closed\",\n",
    "    \"case_opened_date\",\n",
    "    \"case_closed_date\",\n",
    "    \"case_age\",\n",
    "    \"days_to_closed\",\n",
    "    \"case_lifetime\",\n",
    ").where(expr(\"case_closed\")).show(5)\n",
    "\n",
    "df.select(\n",
    "    \"case_closed\",\n",
    "    \"case_opened_date\",\n",
    "    \"case_closed_date\",\n",
    "    \"case_age\",\n",
    "    \"days_to_closed\",\n",
    "    \"case_lifetime\",\n",
    ").where(expr(\"! case_closed\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Department Data\n",
    "\n",
    "We have access to another dataset, `dept.csv`, that contains more information about the various different departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|       dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|     311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|               Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|     Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|Clean and Green N...|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|    Code Enforcement|Code Enforcement ...|  DSD/Code Enforcement|                YES|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = spark.read.csv(\"data/dept.csv\", header=True, inferSchema=True)\n",
    "dept.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be useful to include this data, so we can join it to our case dataframe using the `dept_division` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 2018-01-01 00:42:00  \n",
      " case_closed_date     | 2018-01-01 00:42:00  \n",
      " case_due_date        | 2018-01-01 00:42:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | true                 \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  el paso st,... \n",
      " council_district     | 005                  \n",
      " num_weeks_late       | -142.6441088         \n",
      " zipcode              | 78207                \n",
      " case_age             | 707                  \n",
      " days_to_closed       | 0                    \n",
      " case_lifetime        | 0                    \n",
      " department           | Animal Care Services \n",
      " dept_subject_to_SLA  | true                 \n",
      "-RECORD 1------------------------------------\n",
      " case_id              | 1014127333           \n",
      " case_opened_date     | 2018-01-01 00:46:00  \n",
      " case_closed_date     | 2018-01-01 00:46:00  \n",
      " case_due_date        | 2018-01-01 00:46:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -2.0126041669999997  \n",
      " case_closed          | true                 \n",
      " service_request_type | Removal Of Obstru... \n",
      " SLA_days             | 4.322222222          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 2215  goliad rd, ... \n",
      " council_district     | 003                  \n",
      " num_weeks_late       | -0.28751488099999994 \n",
      " zipcode              | 78223                \n",
      " case_age             | 707                  \n",
      " days_to_closed       | 0                    \n",
      " case_lifetime        | 0                    \n",
      " department           | Trans & Cap Impro... \n",
      " dept_subject_to_SLA  | true                 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    df\n",
    "    # left join on dept_division\n",
    "    .join(dept, \"dept_division\", \"left\")\n",
    "    # drop all the columns except for standardized name, as it has much fewer unique values\n",
    "    .drop(dept.dept_division)\n",
    "    .drop(dept.dept_name)\n",
    "    .drop(df.dept_division)\n",
    "    .withColumnRenamed(\"standardized_dept_name\", \"department\")\n",
    "    # convert to a boolean\n",
    "    .withColumn(\"dept_subject_to_SLA\", col(\"dept_subject_to_SLA\") == \"YES\")\n",
    ")\n",
    "\n",
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2])\n",
    "#\n",
    "train, validate, test = df.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Data Acquisition\n",
    "\n",
    "These exercises should go in a notebook or script named `wrangle`. Add, commit, and push your changes.\n",
    "\n",
    "This exercises uses the `case.csv`, `dept.csv`, and `source.csv` files from the\n",
    "san antonio 311 call dataset.\n",
    "\n",
    "1. Read the case, department, and source data into their own spark dataframes.\n",
    "\n",
    "1. Let's see how writing to the local disk works in spark:\n",
    "\n",
    "    - Write the code necessary to store the source data in both csv and json\n",
    "      format, store these as `sources_csv` and `sources_json`\n",
    "    - Inspect your folder structure. What do you notice?\n",
    "\n",
    "9. Inspect the data in your dataframes. Are the data types appropriate? Write\n",
    "   the code necessary to cast the values to the appropriate types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. How old is the latest (in terms of days past SLA) currently open issue? How\n",
    "   long has the oldest (in terms of days since opened) currently opened issue\n",
    "   been open?\n",
    "1. How many Stray Animal cases are there?\n",
    "1. How many service requests that are assigned to the Field Operations\n",
    "   department (`dept_division`) are not classified as \"Officer Standby\" request\n",
    "   type (`service_request_type`)?\n",
    "\n",
    "1. Convert the `council_district` column to a string column.\n",
    "1. Extract the year from the `case_closed_date` column.\n",
    "1. Convert `num_days_late` from days to hours in new columns `num_hours_late`.\n",
    "\n",
    "1. Join the case data with the source and department data.\n",
    "1. Are there any cases that do not have a request source?\n",
    "\n",
    "1. What are the top 10 service request types in terms of number of requests?\n",
    "1. What are the top 10 service request types in terms of average days late?\n",
    "1. Does number of days late depend on department?\n",
    "1. How do number of days late depend on department and request type?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that the latest date in the dataset is fairly far off from the present day. To account for this, replace any occurances of the current time with the maximum date from the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
